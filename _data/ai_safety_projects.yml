- title: "AI Scheming Project"
  description: "Empirical research project testing whether coding assistants will 'benevolently deceive' clients to avoid implementing insecure code. I tried to set up a somewhat realistic toy environment for iterating monitors, using a 3-turn conversation. [Project Details on LessWrong](https://www.lesswrong.com/posts/rQWHwazYsBi97jMBg/scheming-toy-environment-incompetent-client)"
  images:
    - "images/scheming/czc1tfpo7crr9dhhyfwp.webp"
    - "images/scheming/hqz0ojexafzn0yhsgs0t.webp"
    - "images/scheming/mnja2odatzspjnn2a24a.webp"
    - "images/scheming/w29zxapu9pl9aovlo04y.webp"
    - "images/scheming/ba26jwinak4gthuex2bm.webp"


- title: "AI Standards Lab"
  description: "Co-founded and co-led a nonprofit focused on supporting the development of standards and risk management frameworks for AI systems. We successfully delivered a 150-page contribution to the [EU GPAI Codes of Practice](https://digital-strategy.ec.europa.eu/en/policies/contents-code-gpai), with much of our text directly incorporated into the final document. I also co-authored a [joint letter to the European Commission](https://thefuturesociety.org/wp-content/uploads/2025/06/ProtectingGPAIRules.pdf), signed by two Nobel laureates and prominent academics. Earlier on, we focused on CEN-CENELEC JTC21 standards for \"High-Risk\" systems in the AI Act."
  images:
    - "images/ai standards/AISL Risk Catalog poster.webp"
    - "images/ai standards/IMG_4817.jpg"
    - "images/ai standards/unnamed.png"
    - "images/ai standards/AISL Risk Catalog poster.pdf"

- title: "Courses, fellowships, etc."
  description: |
    • Talos Fellowship - AI Governance (International governance track fellowship with intensive training week in Brussels)
    • Constellation Research Center Fellowship (1-week visiting fellowship)
    • Apart Research METR Hackathon (2 submissions, $4000 in task bounties - code not publishable due to METR contamination policy)
    • AI Safety Lectures & Teaching (Toronto AI Safety Group): [AI Governance Lecture](https://drive.google.com/file/d/1cqccDkDPH72ZjBPl0Q30piXI8Wcvm9_5/view), [Mesaoptimizers Presentation](https://miro.com/app/board/uXjVNlqdxxA=/?share_link_id=208586423568&shareablePresentation=1), [Shard Theory Presentation](https://miro.com/app/board/uXjVK_gqhYk=/?share_link_id=380458724426&shareablePresentation=1), [AI Timelines Presentation](https://docs.google.com/presentation/d/1yzQVsnacJmkkDLdFbIVg1pku6RxZDPawGTVBN6VrFro/edit), [ELK Presentation](https://docs.google.com/presentation/d/1vgDZZ4Dowhq2Uz-75WckPVtNkQJC2nACJuIQKQUgg8M/edit?slide=id.p#slide=id.p), [AI Control Presentation](https://docs.google.com/presentation/d/1HGu5h33bYITNOdQ7aQo0vwglnnRkL_jY7gK9SOKR5mA/edit?slide=id.gc6f980f91_0_0#slide=id.gc6f980f91_0_0)
    • AI Safety Fundamentals 101 & 201 (Alignment Track)